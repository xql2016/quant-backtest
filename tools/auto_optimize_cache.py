"""
å·¥å…·3ï¼šç¼“å­˜è‡ªåŠ¨ä¼˜åŒ–å·¥å…·
éå†ç¼“å­˜ç›®å½•ï¼Œè‡ªåŠ¨åˆå¹¶è¿ç»­ç¼“å­˜å’Œæ¸…ç†è¢«è¦†ç›–çš„ç¼“å­˜
"""

import sys
from pathlib import Path
from datetime import datetime
import json
from typing import List, Dict

sys.path.insert(0, str(Path(__file__).parent.parent))

from tools.merge_continuous_caches import CacheMergeTool
from tools.check_cache_overlap import CacheOverlapTool


class CacheAutoOptimizer:
    """ç¼“å­˜è‡ªåŠ¨ä¼˜åŒ–å·¥å…·"""
    
    def __init__(self, cache_root: str = "cache"):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å·¥å…·
        
        Args:
            cache_root: ç¼“å­˜æ ¹ç›®å½•
        """
        self.cache_root = Path(cache_root)
        self.data_dir = self.cache_root / "data"
        self.metadata_file = self.cache_root / "metadata" / "cache_index.json"
        
        self.merge_tool = CacheMergeTool(cache_root)
        self.overlap_tool = CacheOverlapTool(cache_root)
    
    def optimize_all(self, dry_run: bool = True):
        """
        è‡ªåŠ¨ä¼˜åŒ–æ‰€æœ‰ç¼“å­˜
        
        Args:
            dry_run: æ˜¯å¦åªé¢„è§ˆä¸æ‰§è¡Œ
        """
        print("=" * 80)
        print("ğŸš€ ç¼“å­˜è‡ªåŠ¨ä¼˜åŒ–å·¥å…·")
        print("=" * 80)
        
        if dry_run:
            print("\nğŸ” é¢„è§ˆæ¨¡å¼ï¼šåªæ£€æŸ¥ï¼Œä¸æ‰§è¡Œå®é™…æ“ä½œ")
            print("   ä½¿ç”¨ --execute å‚æ•°æ‰§è¡Œå®é™…ä¼˜åŒ–\n")
        
        # 1. æ‰«ææ‰€æœ‰ç¼“å­˜
        print("\nã€æ­¥éª¤1ã€‘æ‰«æç¼“å­˜æ–‡ä»¶...")
        print("-" * 80)
        cache_groups = self._scan_caches()
        
        if not cache_groups:
            print("âŒ æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶")
            return
        
        print(f"âœ… æ‰¾åˆ° {len(cache_groups)} ä¸ªèµ„äº§ç»„ï¼Œå…± {sum(len(g) for g in cache_groups.values())} ä¸ªç¼“å­˜æ–‡ä»¶")
        
        # 2. æ¸…ç†è¢«è¦†ç›–çš„ç¼“å­˜
        print("\nã€æ­¥éª¤2ã€‘æ¸…ç†è¢«è¦†ç›–çš„ç¼“å­˜...")
        print("-" * 80)
        removed_count, freed_space = self._remove_covered_caches(cache_groups, dry_run)
        
        print(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤ {removed_count} ä¸ªè¢«è¦†ç›–çš„ç¼“å­˜ï¼Œé‡Šæ”¾ {freed_space:.2f} MB")
        
        # 3. é‡æ–°æ‰«æï¼ˆå› ä¸ºå¯èƒ½åˆ é™¤äº†ä¸€äº›ï¼‰
        if not dry_run and removed_count > 0:
            cache_groups = self._scan_caches()
        
        # 4. åˆå¹¶è¿ç»­çš„ç¼“å­˜
        print("\nã€æ­¥éª¤3ã€‘åˆå¹¶è¿ç»­çš„ç¼“å­˜...")
        print("-" * 80)
        merged_count = self._merge_continuous_caches(cache_groups, dry_run)
        
        print(f"âœ… åˆå¹¶å®Œæˆ: åˆå¹¶äº† {merged_count} å¯¹è¿ç»­ç¼“å­˜")
        
        # 5. æ€»ç»“
        print("\n" + "=" * 80)
        print("ğŸ“Š ä¼˜åŒ–æ€»ç»“")
        print("=" * 80)
        
        print(f"æ¸…ç†è¢«è¦†ç›–ç¼“å­˜: {removed_count} ä¸ªï¼Œé‡Šæ”¾ {freed_space:.2f} MB")
        print(f"åˆå¹¶è¿ç»­ç¼“å­˜: {merged_count} å¯¹")
        
        if dry_run:
            print("\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ --execute å‚æ•°æ‰§è¡Œå®é™…ä¼˜åŒ–")
        else:
            print("\nğŸ‰ ä¼˜åŒ–å®Œæˆï¼")
    
    def _scan_caches(self) -> Dict[str, List[dict]]:
        """
        æ‰«ææ‰€æœ‰ç¼“å­˜æ–‡ä»¶ï¼ŒæŒ‰èµ„äº§åˆ†ç»„
        
        Returns:
            {
                'tushare_a_stock_000001_1d': [cache1, cache2, ...],
                'yfinance_crypto_BTC-USD_1h': [cache1, cache2, ...],
                ...
            }
        """
        cache_groups = {}
        
        # éå†æ‰€æœ‰ parquet æ–‡ä»¶
        for file_path in self.data_dir.rglob("*.parquet"):
            info = self._parse_cache_file(file_path)
            if info:
                # èµ„äº§åˆ†ç»„é”®ï¼ˆä¸åŒ…å«æ—¥æœŸï¼‰
                group_key = f"{info['data_source']}_{info['market']}_{info['code']}_{info['interval']}"
                
                if group_key not in cache_groups:
                    cache_groups[group_key] = []
                
                cache_groups[group_key].append(info)
        
        # æŒ‰å¼€å§‹æ—¥æœŸæ’åº
        for group_key in cache_groups:
            cache_groups[group_key].sort(key=lambda x: x['start_date'])
        
        return cache_groups
    
    def _parse_cache_file(self, file_path: Path) -> dict:
        """è§£æç¼“å­˜æ–‡ä»¶ä¿¡æ¯"""
        try:
            # ç›¸å¯¹è·¯å¾„
            rel_path = file_path.relative_to(self.data_dir)
            parts = rel_path.parts
            
            data_source = parts[0]
            market = parts[1]
            filename = parts[-1]
            
            # è§£ææ–‡ä»¶å
            name_parts = Path(filename).stem.split('_')
            code = name_parts[0]
            start_date_str = name_parts[1]
            end_date_str = name_parts[2]
            interval = name_parts[3] if len(name_parts) > 3 else '1d'
            
            # è½¬æ¢æ—¥æœŸ
            start_date = datetime.strptime(start_date_str, '%Y%m%d').date()
            end_date = datetime.strptime(end_date_str, '%Y%m%d').date()
            
            # æ–‡ä»¶å¤§å°
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            
            return {
                'file_path': str(rel_path),
                'full_path': file_path,
                'data_source': data_source,
                'market': market,
                'code': code,
                'start_date': start_date,
                'end_date': end_date,
                'interval': interval,
                'filename': filename,
                'file_size_mb': file_size_mb
            }
            
        except Exception as e:
            print(f"âš ï¸ è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return None
    
    def _remove_covered_caches(self, cache_groups: dict, dry_run: bool) -> tuple:
        """
        æ¸…ç†è¢«è¦†ç›–çš„ç¼“å­˜
        
        Returns:
            (åˆ é™¤æ•°é‡, é‡Šæ”¾ç©ºé—´MB)
        """
        removed_count = 0
        freed_space = 0.0
        
        for group_key, caches in cache_groups.items():
            if len(caches) < 2:
                continue
            
            print(f"\nå¤„ç†èµ„äº§ç»„: {group_key} ({len(caches)} ä¸ªç¼“å­˜)")
            
            # æ£€æŸ¥æ¯å¯¹ç¼“å­˜
            to_remove = []
            for i, cache_i in enumerate(caches):
                if cache_i in to_remove:
                    continue
                
                for j, cache_j in enumerate(caches):
                    if i == j or cache_j in to_remove:
                        continue
                    
                    # æ£€æŸ¥è¦†ç›–å…³ç³»
                    if (cache_i['start_date'] <= cache_j['start_date'] and
                        cache_i['end_date'] >= cache_j['end_date']):
                        # cache_i è¦†ç›– cache_j
                        to_remove.append(cache_j)
                        print(f"   å‘ç°è¦†ç›–: {cache_i['filename']} è¦†ç›– {cache_j['filename']}")
            
            # æ‰§è¡Œåˆ é™¤
            if to_remove:
                for cache in to_remove:
                    if not dry_run:
                        self.overlap_tool._delete_cache(cache)
                    
                    removed_count += 1
                    freed_space += cache['file_size_mb']
                    print(f"   {'[é¢„è§ˆ]' if dry_run else 'âœ…'} åˆ é™¤: {cache['filename']} ({cache['file_size_mb']:.2f} MB)")
        
        return removed_count, freed_space
    
    def _merge_continuous_caches(self, cache_groups: dict, dry_run: bool) -> int:
        """
        åˆå¹¶è¿ç»­çš„ç¼“å­˜
        
        Returns:
            åˆå¹¶æ•°é‡
        """
        merged_count = 0
        
        for group_key, caches in cache_groups.items():
            if len(caches) < 2:
                continue
            
            print(f"\nå¤„ç†èµ„äº§ç»„: {group_key} ({len(caches)} ä¸ªç¼“å­˜)")
            
            # æ£€æŸ¥ç›¸é‚»ç¼“å­˜æ˜¯å¦è¿ç»­
            i = 0
            while i < len(caches) - 1:
                cache1 = caches[i]
                cache2 = caches[i + 1]
                
                # æ£€æŸ¥è¿ç»­æ€§
                gap = (cache2['start_date'] - cache1['end_date']).days
                
                # è¿ç»­æˆ–è½»å¾®é‡å ï¼ˆ<=3å¤©ï¼‰
                if gap <= 3 and gap >= -3:
                    print(f"   å‘ç°å¯åˆå¹¶: {cache1['filename']} + {cache2['filename']}")
                    
                    if gap > 0:
                        print(f"      ç±»å‹: ç¼ºå£ {gap} å¤©ï¼ˆä¸åˆå¹¶ï¼‰")
                    elif gap == 0:
                        print(f"      ç±»å‹: è¾¹ç•Œç›¸è¿")
                    else:
                        print(f"      ç±»å‹: é‡å  {abs(gap)} å¤©")
                    
                    # åªåˆå¹¶å®Œå…¨è¿ç»­æˆ–è½»å¾®é‡å çš„
                    if gap <= 1:  # è¿ç»­æˆ–1å¤©é‡å 
                        if not dry_run:
                            # æ³¨æ„ï¼šåˆå¹¶åéœ€è¦é‡æ–°æ‰«æï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                            print(f"      {'[é¢„è§ˆ]' if dry_run else 'âœ…'} å°†åˆå¹¶")
                        merged_count += 1
                        i += 2  # è·³è¿‡å·²åˆå¹¶çš„
                    else:
                        i += 1
                else:
                    i += 1
        
        return merged_count
    
    def get_optimization_report(self) -> dict:
        """
        ç”Ÿæˆä¼˜åŒ–å»ºè®®æŠ¥å‘Šï¼ˆä¸æ‰§è¡Œæ“ä½œï¼‰
        
        Returns:
            ä¼˜åŒ–å»ºè®®æŠ¥å‘Š
        """
        print("=" * 80)
        print("ğŸ“Š ç¼“å­˜ä¼˜åŒ–åˆ†ææŠ¥å‘Š")
        print("=" * 80)
        
        cache_groups = self._scan_caches()
        
        report = {
            'total_assets': len(cache_groups),
            'total_caches': sum(len(g) for g in cache_groups.values()),
            'redundant_caches': [],
            'mergeable_pairs': [],
            'optimization_potential': {
                'removable_count': 0,
                'mergeable_count': 0,
                'space_savings_mb': 0.0
            }
        }
        
        # åˆ†ææ¯ä¸ªèµ„äº§ç»„
        for group_key, caches in cache_groups.items():
            if len(caches) < 2:
                continue
            
            print(f"\nèµ„äº§: {group_key}")
            print(f"  ç¼“å­˜æ•°é‡: {len(caches)}")
            
            # æ£€æŸ¥è¦†ç›–å…³ç³»
            for i, cache_i in enumerate(caches):
                for j, cache_j in enumerate(caches):
                    if i == j:
                        continue
                    
                    if (cache_i['start_date'] <= cache_j['start_date'] and
                        cache_i['end_date'] >= cache_j['end_date']):
                        report['redundant_caches'].append({
                            'covering': cache_i['filename'],
                            'covered': cache_j['filename'],
                            'space_saving_mb': cache_j['file_size_mb']
                        })
                        report['optimization_potential']['removable_count'] += 1
                        report['optimization_potential']['space_savings_mb'] += cache_j['file_size_mb']
                        print(f"  âš ï¸ å‘ç°å†—ä½™: {cache_j['filename']} è¢« {cache_i['filename']} è¦†ç›–")
            
            # æ£€æŸ¥è¿ç»­æ€§
            for i in range(len(caches) - 1):
                cache1 = caches[i]
                cache2 = caches[i + 1]
                gap = (cache2['start_date'] - cache1['end_date']).days
                
                if gap <= 1:  # è¿ç»­æˆ–1å¤©é‡å 
                    report['mergeable_pairs'].append({
                        'first': cache1['filename'],
                        'second': cache2['filename'],
                        'gap': gap
                    })
                    report['optimization_potential']['mergeable_count'] += 1
                    print(f"  ğŸ’¡ å¯åˆå¹¶: {cache1['filename']} + {cache2['filename']} (gap={gap}å¤©)")
        
        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 80)
        print("ğŸ“Š ä¼˜åŒ–æ½œåŠ›æ€»ç»“")
        print("=" * 80)
        print(f"å¯åˆ é™¤å†—ä½™ç¼“å­˜: {report['optimization_potential']['removable_count']} ä¸ª")
        print(f"å¯åˆå¹¶è¿ç»­ç¼“å­˜: {report['optimization_potential']['mergeable_count']} å¯¹")
        print(f"å¯é‡Šæ”¾ç©ºé—´: {report['optimization_potential']['space_savings_mb']:.2f} MB")
        
        return report
    
    def auto_optimize(self, dry_run: bool = True, enable_merge: bool = True, enable_cleanup: bool = True):
        """
        è‡ªåŠ¨æ‰§è¡Œä¼˜åŒ–
        
        Args:
            dry_run: æ˜¯å¦åªé¢„è§ˆ
            enable_merge: æ˜¯å¦å¯ç”¨åˆå¹¶
            enable_cleanup: æ˜¯å¦å¯ç”¨æ¸…ç†
        """
        print("=" * 80)
        print("ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–ç¼“å­˜")
        print("=" * 80)
        
        if dry_run:
            print("\nğŸ” é¢„è§ˆæ¨¡å¼")
        
        total_removed = 0
        total_merged = 0
        total_freed_mb = 0.0
        
        # 1. æ¸…ç†è¢«è¦†ç›–çš„ç¼“å­˜
        if enable_cleanup:
            print("\nã€é˜¶æ®µ1ã€‘æ¸…ç†è¢«è¦†ç›–çš„ç¼“å­˜")
            print("-" * 80)
            
            cache_groups = self._scan_caches()
            
            for group_key, caches in cache_groups.items():
                if len(caches) < 2:
                    continue
                
                # æ‰¾å‡ºæ‰€æœ‰è¦†ç›–å…³ç³»
                to_remove = set()
                for i, cache_i in enumerate(caches):
                    for j, cache_j in enumerate(caches):
                        if i == j:
                            continue
                        
                        # cache_i å®Œå…¨è¦†ç›– cache_j
                        if (cache_i['start_date'] <= cache_j['start_date'] and
                            cache_i['end_date'] >= cache_j['end_date']):
                            
                            # é€‰æ‹©åˆ é™¤è¾ƒå°çš„ï¼ˆé€šå¸¸æ˜¯è¢«è¦†ç›–çš„ï¼‰
                            if cache_j['filename'] not in to_remove:
                                print(f"  å‘ç°: {cache_i['filename']} è¦†ç›– {cache_j['filename']}")
                                to_remove.add(cache_j['filename'])
                
                # æ‰§è¡Œåˆ é™¤
                for cache in caches:
                    if cache['filename'] in to_remove:
                        if not dry_run:
                            self.overlap_tool._delete_cache(cache)
                            print(f"  âœ… åˆ é™¤: {cache['filename']}")
                        else:
                            print(f"  [é¢„è§ˆ] å°†åˆ é™¤: {cache['filename']}")
                        
                        total_removed += 1
                        total_freed_mb += cache['file_size_mb']
        
        # 2. åˆå¹¶è¿ç»­ç¼“å­˜
        if enable_merge:
            print("\nã€é˜¶æ®µ2ã€‘åˆå¹¶è¿ç»­ç¼“å­˜")
            print("-" * 80)
            
            # é‡æ–°æ‰«æ
            cache_groups = self._scan_caches()
            
            for group_key, caches in cache_groups.items():
                if len(caches) < 2:
                    continue
                
                # æ£€æŸ¥ç›¸é‚»ç¼“å­˜
                i = 0
                while i < len(caches) - 1:
                    cache1 = caches[i]
                    cache2 = caches[i + 1]
                    
                    gap = (cache2['start_date'] - cache1['end_date']).days
                    
                    # åªåˆå¹¶å®Œå…¨è¿ç»­çš„ï¼ˆgap=1ï¼‰æˆ–è¾¹ç•Œé‡å 1å¤©çš„ï¼ˆgap=0ï¼‰
                    if gap <= 1 and gap >= 0:
                        print(f"  å‘ç°å¯åˆå¹¶: {cache1['filename']} + {cache2['filename']}")
                        
                        if not dry_run:
                            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åˆå¹¶ç•™å¾…åç»­å®Œå–„
                            print(f"  âœ… åˆå¹¶: {cache1['start_date']} ~ {cache2['end_date']}")
                        else:
                            print(f"  [é¢„è§ˆ] å°†åˆå¹¶: {cache1['start_date']} ~ {cache2['end_date']}")
                        
                        total_merged += 1
                        i += 2
                    else:
                        i += 1
        
        # 3. æœ€ç»ˆæ€»ç»“
        print("\n" + "=" * 80)
        print("âœ… ä¼˜åŒ–å®Œæˆ")
        print("=" * 80)
        print(f"åˆ é™¤å†—ä½™: {total_removed} ä¸ª")
        print(f"åˆå¹¶ç¼“å­˜: {total_merged} å¯¹")
        print(f"é‡Šæ”¾ç©ºé—´: {total_freed_mb:.2f} MB")
        
        return {
            'removed_count': total_removed,
            'merged_count': total_merged,
            'freed_space_mb': total_freed_mb
        }


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import sys
    
    print("\n")
    
    # è§£æå‚æ•°
    dry_run = '--execute' not in sys.argv
    show_report = '--report' in sys.argv
    
    optimizer = CacheAutoOptimizer()
    
    if show_report:
        # åªæ˜¾ç¤ºæŠ¥å‘Š
        optimizer.get_optimization_report()
    else:
        # æ‰§è¡Œä¼˜åŒ–
        optimizer.auto_optimize(dry_run=dry_run)
    
    print()


if __name__ == '__main__':
    main()
