"""
å·¥å…·1ï¼šä¸¤ç¼“å­˜è¿ç»­åˆå¹¶å·¥å…·
åˆ¤æ–­ä¸¤ä¸ªç¼“å­˜æ˜¯å¦å®Œå…¨è¿ç»­ï¼Œå¦‚æœæ˜¯åˆ™åˆå¹¶å¹¶åˆ é™¤åŸç¼“å­˜
"""

import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta, date
import sys
import json

sys.path.insert(0, str(Path(__file__).parent.parent))


class CacheMergeTool:
    """ç¼“å­˜åˆå¹¶å·¥å…·"""
    
    def __init__(self, cache_root: str = "cache"):
        """
        åˆå§‹åŒ–åˆå¹¶å·¥å…·
        
        Args:
            cache_root: ç¼“å­˜æ ¹ç›®å½•
        """
        self.cache_root = Path(cache_root)
        self.data_dir = self.cache_root / "data"
        self.metadata_file = self.cache_root / "metadata" / "cache_index.json"
    
    def merge_continuous_caches(self, file1: str, file2: str, dry_run: bool = False) -> dict:
        """
        åˆå¹¶ä¸¤ä¸ªè¿ç»­çš„ç¼“å­˜
        
        Args:
            file1: ç¬¬ä¸€ä¸ªç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºcache/data/ï¼‰
            file2: ç¬¬äºŒä¸ªç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºcache/data/ï¼‰
            dry_run: æ˜¯å¦åªæ£€æŸ¥ä¸æ‰§è¡Œï¼ˆé¢„è§ˆæ¨¡å¼ï¼‰
            
        Returns:
            ç»“æœå­—å…¸ï¼ŒåŒ…å«statuså’Œmessage
        """
        print("=" * 80)
        print("ğŸ”„ ä¸¤ç¼“å­˜è¿ç»­åˆå¹¶å·¥å…·")
        print("=" * 80)
        
        # 1. è§£ææ–‡ä»¶ä¿¡æ¯
        info1 = self._parse_cache_file(file1)
        info2 = self._parse_cache_file(file2)
        
        if not info1 or not info2:
            return {
                'status': 'error',
                'message': 'æ–‡ä»¶ä¿¡æ¯è§£æå¤±è´¥'
            }
        
        print(f"\nç¼“å­˜A: {info1['code']} ({info1['start_date']} ~ {info1['end_date']})")
        print(f"ç¼“å­˜B: {info2['code']} ({info2['start_date']} ~ {info2['end_date']})")
        
        # 2. éªŒè¯åŸºæœ¬ä¿¡æ¯ä¸€è‡´
        if not self._validate_same_asset(info1, info2):
            return {
                'status': 'error',
                'message': 'ä¸¤ä¸ªç¼“å­˜ä¸æ˜¯åŒä¸€ä¸ªèµ„äº§ï¼ˆæ•°æ®æºã€å¸‚åœºã€ä»£ç æˆ–æ—¶é—´ç²’åº¦ä¸åŒï¼‰'
            }
        
        print("âœ… åŸºæœ¬ä¿¡æ¯éªŒè¯é€šè¿‡ï¼ˆåŒä¸€èµ„äº§ï¼‰")
        
        # 3. æ£€æŸ¥è¿ç»­æ€§
        continuity_result = self._check_continuity(info1, info2)
        
        if not continuity_result['is_continuous']:
            return {
                'status': 'error',
                'message': continuity_result['message']
            }
        
        print(f"âœ… è¿ç»­æ€§æ£€æŸ¥é€šè¿‡: {continuity_result['message']}")
        
        # 4. ç¡®å®šåˆå¹¶é¡ºåºå’Œå¤„ç†é‡å 
        merge_plan = self._plan_merge(info1, info2, continuity_result)
        
        print(f"\nğŸ“‹ åˆå¹¶è®¡åˆ’:")
        print(f"   æ—¶é—´é¡ºåº: ç¼“å­˜{merge_plan['first']} â†’ ç¼“å­˜{merge_plan['second']}")
        print(f"   åˆå¹¶èŒƒå›´: {merge_plan['start_date']} ~ {merge_plan['end_date']}")
        print(f"   é‡å å¤„ç†: {merge_plan['overlap_strategy']}")
        
        if dry_run:
            print("\nğŸ” é¢„è§ˆæ¨¡å¼ï¼šä¸æ‰§è¡Œå®é™…åˆå¹¶")
            return {
                'status': 'preview',
                'message': 'é¢„è§ˆæˆåŠŸï¼Œä½¿ç”¨ dry_run=False æ‰§è¡Œå®é™…åˆå¹¶',
                'plan': merge_plan
            }
        
        # 5. æ‰§è¡Œåˆå¹¶
        print(f"\nğŸ”§ å¼€å§‹åˆå¹¶...")
        merge_result = self._execute_merge(info1, info2, merge_plan)
        
        if not merge_result['success']:
            return {
                'status': 'error',
                'message': f"åˆå¹¶å¤±è´¥: {merge_result['message']}"
            }
        
        print(f"âœ… æ•°æ®åˆå¹¶æˆåŠŸ: {merge_result['rows']} æ¡è®°å½•")
        print(f"   ä¿å­˜è·¯å¾„: {merge_result['file_path']}")
        
        # 6. æ›´æ–°ç´¢å¼•
        self._update_index_after_merge(merge_result, merge_plan)
        print(f"âœ… ç´¢å¼•æ›´æ–°æˆåŠŸ")
        
        # 7. åˆ é™¤åŸç¼“å­˜
        self._delete_original_caches(file1, file2)
        print(f"âœ… åŸç¼“å­˜å·²åˆ é™¤")
        
        print("\n" + "=" * 80)
        print("ğŸ‰ åˆå¹¶å®Œæˆï¼")
        print("=" * 80)
        
        return {
            'status': 'success',
            'message': 'åˆå¹¶æˆåŠŸ',
            'merged_file': merge_result['file_path'],
            'merged_rows': merge_result['rows']
        }
    
    def _parse_cache_file(self, file_path: str) -> dict:
        """
        ä»æ–‡ä»¶åè§£æç¼“å­˜ä¿¡æ¯
        
        æ–‡ä»¶åæ ¼å¼: {code}_{start_date}_{end_date}_{interval}.parquet
        æˆ–: {code}_{start_date}_{end_date}.parquet
        """
        try:
            file_path = Path(file_path)
            full_path = self.data_dir / file_path
            
            if not full_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
                return None
            
            # è§£æè·¯å¾„
            parts = file_path.parts
            if len(parts) < 3:
                print(f"âŒ æ–‡ä»¶è·¯å¾„æ ¼å¼é”™è¯¯: {file_path}")
                return None
            
            data_source = parts[0]  # akshare/yfinance/tushare
            market = parts[1]       # a_stock/hk_stock/etc
            filename = parts[-1]    # æ–‡ä»¶å
            
            # è§£ææ–‡ä»¶å
            name_parts = Path(filename).stem.split('_')
            if len(name_parts) < 3:
                print(f"âŒ æ–‡ä»¶åæ ¼å¼é”™è¯¯: {filename}")
                return None
            
            code = name_parts[0]
            start_date_str = name_parts[1]
            end_date_str = name_parts[2]
            interval = name_parts[3] if len(name_parts) > 3 else '1d'
            
            # è½¬æ¢æ—¥æœŸ
            start_date = datetime.strptime(start_date_str, '%Y%m%d').date()
            end_date = datetime.strptime(end_date_str, '%Y%m%d').date()
            
            return {
                'file_path': str(file_path),
                'full_path': full_path,
                'data_source': data_source,
                'market': market,
                'code': code,
                'start_date': start_date,
                'end_date': end_date,
                'interval': interval,
                'filename': filename
            }
            
        except Exception as e:
            print(f"âŒ è§£ææ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _validate_same_asset(self, info1: dict, info2: dict) -> bool:
        """éªŒè¯æ˜¯å¦æ˜¯åŒä¸€èµ„äº§"""
        return (info1['data_source'] == info2['data_source'] and
                info1['market'] == info2['market'] and
                info1['code'] == info2['code'] and
                info1['interval'] == info2['interval'])
    
    def _check_continuity(self, info1: dict, info2: dict) -> dict:
        """
        æ£€æŸ¥ä¸¤ä¸ªç¼“å­˜æ˜¯å¦è¿ç»­
        
        Returns:
            {
                'is_continuous': bool,
                'type': 'continuous' | 'overlap' | 'gap',
                'message': str
            }
        """
        # ç¡®å®šæ—¶é—´é¡ºåº
        if info1['start_date'] <= info2['start_date']:
            first, second = info1, info2
        else:
            first, second = info2, info1
        
        # è®¡ç®—æ—¶é—´å·®
        gap = (second['start_date'] - first['end_date']).days
        
        if gap == 1:
            # å®Œå…¨è¿ç»­ï¼ˆä¸­é—´æ— ç¼ºå£ï¼Œæ— é‡å ï¼‰
            return {
                'is_continuous': True,
                'type': 'continuous',
                'message': f"å®Œå…¨è¿ç»­ï¼ˆç¼“å­˜Aåˆ°{first['end_date']}ï¼Œç¼“å­˜Bä»{second['start_date']}ï¼‰",
                'gap_days': 0
            }
        elif gap == 0:
            # è¾¹ç•Œé‡å 1å¤©
            return {
                'is_continuous': True,
                'type': 'overlap',
                'message': f"è¾¹ç•Œæ—¥æœŸé‡å : {second['start_date']}",
                'overlap_start': second['start_date'],
                'overlap_end': min(first['end_date'], second['end_date'])
            }
        elif gap < 0:
            # å¤šå¤©é‡å 
            overlap_days = abs(gap) + 1
            return {
                'is_continuous': True,
                'type': 'overlap',
                'message': f"æ—¥æœŸé‡å  {overlap_days} å¤© ({second['start_date']} ~ {first['end_date']})",
                'overlap_start': second['start_date'],
                'overlap_end': first['end_date']
            }
        else:
            # å­˜åœ¨ç¼ºå£
            return {
                'is_continuous': False,
                'type': 'gap',
                'message': f"å­˜åœ¨ {gap} å¤©ç¼ºå£ ({first['end_date']} åˆ° {second['start_date']})",
                'gap_days': gap
            }
    
    def _plan_merge(self, info1: dict, info2: dict, continuity: dict) -> dict:
        """åˆ¶å®šåˆå¹¶è®¡åˆ’"""
        # ç¡®å®šé¡ºåº
        if info1['start_date'] <= info2['start_date']:
            first, second = info1, info2
            first_label, second_label = 'A', 'B'
        else:
            first, second = info2, info1
            first_label, second_label = 'B', 'A'
        
        # åˆå¹¶åçš„æ—¥æœŸèŒƒå›´
        start_date = first['start_date']
        end_date = second['end_date']
        
        # é‡å å¤„ç†ç­–ç•¥
        if continuity['type'] == 'overlap':
            overlap_strategy = "ä½¿ç”¨ç¼“å­˜Bçš„æ•°æ®ï¼ˆæ›´æ–°ï¼‰"
        else:
            overlap_strategy = "æ— é‡å "
        
        return {
            'first': first_label,
            'second': second_label,
            'first_info': first,
            'second_info': second,
            'start_date': start_date,
            'end_date': end_date,
            'overlap_strategy': overlap_strategy,
            'continuity_type': continuity['type']
        }
    
    def _execute_merge(self, info1: dict, info2: dict, plan: dict) -> dict:
        """æ‰§è¡Œåˆå¹¶æ“ä½œ"""
        try:
            first_info = plan['first_info']
            second_info = plan['second_info']
            
            # è¯»å–æ•°æ®
            df1 = pd.read_parquet(first_info['full_path'])
            df2 = pd.read_parquet(second_info['full_path'])
            
            print(f"   è¯»å–ç¼“å­˜A: {len(df1)} æ¡è®°å½•")
            print(f"   è¯»å–ç¼“å­˜B: {len(df2)} æ¡è®°å½•")
            
            # å¤„ç†é‡å 
            if plan['continuity_type'] == 'overlap':
                # ä»df1ä¸­åˆ é™¤é‡å éƒ¨åˆ†ï¼Œä½¿ç”¨df2çš„æ•°æ®
                overlap_start = second_info['start_date']
                df1_filtered = df1[df1.index.date < overlap_start]
                print(f"   å¤„ç†é‡å : ä¿ç•™ç¼“å­˜A {len(df1) - len(df1_filtered)} æ¡ï¼Œä½¿ç”¨ç¼“å­˜Bçš„æ•°æ®")
            else:
                df1_filtered = df1
            
            # åˆå¹¶
            merged_df = pd.concat([df1_filtered, df2])
            merged_df = merged_df.sort_index()
            
            # å»é‡ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
            if merged_df.index.duplicated().any():
                print(f"   âš ï¸ å‘ç°é‡å¤æ—¥æœŸï¼Œå»é‡å¤„ç†")
                merged_df = merged_df[~merged_df.index.duplicated(keep='last')]
            
            # ç”Ÿæˆåˆå¹¶åçš„æ–‡ä»¶è·¯å¾„
            merged_file_path = self._generate_merged_file_path(
                first_info, plan['start_date'], plan['end_date']
            )
            
            # ä¿å­˜åˆå¹¶åçš„æ•°æ®
            merged_df.to_parquet(merged_file_path, compression='snappy')
            
            return {
                'success': True,
                'file_path': str(merged_file_path),
                'rows': len(merged_df),
                'start_date': merged_df.index[0].date(),
                'end_date': merged_df.index[-1].date(),
                'message': 'åˆå¹¶æˆåŠŸ'
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f'åˆå¹¶å¤±è´¥: {e}'
            }
    
    def _generate_merged_file_path(self, info: dict, start_date: date, end_date: date) -> Path:
        """ç”Ÿæˆåˆå¹¶åçš„æ–‡ä»¶è·¯å¾„"""
        subdir = self.data_dir / info['data_source'] / info['market']
        
        start_str = start_date.strftime('%Y%m%d')
        end_str = end_date.strftime('%Y%m%d')
        
        if info['interval'] == '1d':
            filename = f"{info['code']}_{start_str}_{end_str}.parquet"
        else:
            filename = f"{info['code']}_{start_str}_{end_str}_{info['interval']}.parquet"
        
        return subdir / filename
    
    def _update_index_after_merge(self, merge_result: dict, plan: dict):
        """æ›´æ–°ç´¢å¼•æ–‡ä»¶"""
        try:
            # è¯»å–ç´¢å¼•
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            # ç”Ÿæˆæ–°çš„ç¼“å­˜é”®
            first_info = plan['first_info']
            start_str = plan['start_date'].strftime('%Y%m%d')
            end_str = plan['end_date'].strftime('%Y%m%d')
            
            new_key = f"{first_info['data_source']}_{first_info['market']}_{first_info['code']}_{start_str}_{end_str}_{first_info['interval']}"
            
            # åˆ›å»ºæ–°çš„å…ƒæ•°æ®
            file_path = Path(merge_result['file_path'])
            metadata = {
                'file_path': merge_result['file_path'],
                'data_source': first_info['data_source'],
                'market': first_info['market'],
                'code': first_info['code'],
                'start_date': str(plan['start_date']),
                'end_date': str(plan['end_date']),
                'interval': first_info['interval'],
                'rows': merge_result['rows'],
                'columns': ['open', 'high', 'low', 'close', 'volume'],
                'created_at': datetime.now().isoformat(),
                'last_accessed': datetime.now().isoformat(),
                'access_count': 0,
                'file_size_kb': round(file_path.stat().st_size / 1024, 2),
                'checksum': f"md5:merged",
                'is_complete': True,
                'merged_from': [first_info['filename'], plan['second_info']['filename']]
            }
            
            # æ·»åŠ æ–°æ¡ç›®
            index_data['entries'][new_key] = metadata
            
            # åˆ é™¤æ—§æ¡ç›®
            old_keys = [k for k, v in index_data['entries'].items() 
                       if v.get('file_path') in [first_info['file_path'], plan['second_info']['file_path']]]
            for old_key in old_keys:
                del index_data['entries'][old_key]
            
            # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            if index_data['entries']:
                total_size = sum(e.get('file_size_kb', 0) for e in index_data['entries'].values()) / 1024
                created_times = [e['created_at'] for e in index_data['entries'].values() if 'created_at' in e]
                
                index_data['statistics'] = {
                    'total_entries': len(index_data['entries']),
                    'total_size_mb': round(total_size, 2),
                    'oldest_entry': min(created_times) if created_times else None,
                    'newest_entry': max(created_times) if created_times else None
                }
            else:
                index_data['statistics'] = {
                    'total_entries': 0,
                    'total_size_mb': 0.0,
                    'oldest_entry': None,
                    'newest_entry': None
                }
            
            # æ›´æ–°æ—¶é—´æˆ³
            index_data['last_update'] = datetime.now().isoformat()
            
            # ä¿å­˜
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, indent=2, ensure_ascii=False)
            
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°ç´¢å¼•å¤±è´¥: {e}")
    
    def _delete_original_caches(self, file1: str, file2: str):
        """åˆ é™¤åŸå§‹ç¼“å­˜æ–‡ä»¶"""
        try:
            path1 = self.data_dir / file1
            path2 = self.data_dir / file2
            
            if path1.exists():
                path1.unlink()
                print(f"   åˆ é™¤: {file1}")
            
            if path2.exists():
                path2.unlink()
                print(f"   åˆ é™¤: {file2}")
                
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤åŸæ–‡ä»¶å¤±è´¥: {e}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python merge_continuous_caches.py <æ–‡ä»¶1> <æ–‡ä»¶2> [--dry-run]")
        print()
        print("ç¤ºä¾‹:")
        print("  python tools/merge_continuous_caches.py \\")
        print("    tushare/a_stock/000001_20250101_20250708.parquet \\")
        print("    tushare/a_stock/000001_20250602_20260101.parquet")
        print()
        print("é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰:")
        print("  python tools/merge_continuous_caches.py <æ–‡ä»¶1> <æ–‡ä»¶2> --dry-run")
        sys.exit(1)
    
    file1 = sys.argv[1]
    file2 = sys.argv[2]
    dry_run = '--dry-run' in sys.argv
    
    tool = CacheMergeTool()
    result = tool.merge_continuous_caches(file1, file2, dry_run=dry_run)
    
    print(f"\nç»“æœ: {result['status']}")
    print(f"ä¿¡æ¯: {result['message']}")


if __name__ == '__main__':
    main()
