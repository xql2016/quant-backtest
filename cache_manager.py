"""
æ•°æ®ç¼“å­˜ç®¡ç†æ¨¡å—
æä¾›æœ¬åœ°æ–‡ä»¶ç¼“å­˜åŠŸèƒ½ï¼Œå‡å°‘å¯¹ä¸ç¨³å®šAPIçš„ä¾èµ–
"""

import pandas as pd
import os
import json
import hashlib
from datetime import datetime, timedelta, date
from typing import Optional, Dict, List, Tuple
import logging
from pathlib import Path


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - ç»Ÿä¸€çš„ç¼“å­˜å…¥å£"""
    
    def __init__(self, cache_root: str = "cache"):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_root: ç¼“å­˜æ ¹ç›®å½•è·¯å¾„
        """
        self.cache_root = Path(cache_root)
        self.data_dir = self.cache_root / "data"
        self.metadata_dir = self.cache_root / "metadata"
        self.logs_dir = self.cache_root / "logs"
        
        # åŠ è½½é…ç½®
        self.config = self._load_config()
        
        # åˆå§‹åŒ–ç¼“å­˜ç´¢å¼•
        self.index = CacheIndex(self.metadata_dir / "cache_index.json")
        
        # åˆå§‹åŒ–å­˜å‚¨å±‚
        self.storage = CacheStorage(self.data_dir, self.config)
        
        # åˆå§‹åŒ–ç­–ç•¥ç®¡ç†å™¨
        self.policy = CachePolicy(self.config)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
        
        self.logger.info("ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = self.cache_root / "config.json"
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.warning(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        except Exception as e:
            self.logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _get_default_config(self) -> dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "cache_settings": {
                "enabled": True,
                "max_size_mb": 1024,
                "default_ttl_hours": 168
            },
            "storage_format": {
                "format": "parquet",
                "compression": "snappy"
            },
            "logging": {
                "enabled": True,
                "log_level": "INFO"
            }
        }
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        self.logger = logging.getLogger("CacheManager")
        
        if not self.config.get("logging", {}).get("enabled", True):
            self.logger.disabled = True
            return
        
        log_level = self.config.get("logging", {}).get("log_level", "INFO")
        self.logger.setLevel(getattr(logging, log_level))
        
        # é¿å…é‡å¤æ·»åŠ handler
        if not self.logger.handlers:
            # æ§åˆ¶å°handler
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
            
            # æ–‡ä»¶handler
            if self.config.get("logging", {}).get("log_access", False):
                self.logs_dir.mkdir(parents=True, exist_ok=True)
                log_file = self.logs_dir / "cache_access.log"
                file_handler = logging.FileHandler(log_file, encoding='utf-8')
                file_handler.setLevel(logging.DEBUG)
                file_handler.setFormatter(formatter)
                self.logger.addHandler(file_handler)
    
    def get_data(self, 
                 data_source: str,
                 market: str,
                 code: str,
                 start_date: date,
                 end_date: date,
                 interval: str = '1d') -> Optional[pd.DataFrame]:
        """
        è·å–æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            data_source: æ•°æ®æºåç§° ('akshare', 'yfinance', 'tushare')
            market: å¸‚åœºç±»å‹ ('a_stock', 'hk_stock', 'us_stock', 'convertible_bond', 'crypto')
            code: è‚¡ç¥¨/èµ„äº§ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            interval: æ—¶é—´ç²’åº¦ ('1d', '1h', '4h')
            
        Returns:
            DataFrameæˆ–None
        """
        if not self.config.get("cache_settings", {}).get("enabled", True):
            self.logger.info("ç¼“å­˜å·²ç¦ç”¨")
            return None
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(data_source, market, code, start_date, end_date, interval)
        
        self.logger.info(f"æŸ¥è¯¢ç¼“å­˜: {cache_key}")
        
        # æŸ¥è¯¢ç¼“å­˜
        cache_result = self._query_cache(cache_key, start_date, end_date)
        
        if cache_result['status'] == 'full_match':
            self.logger.info(f"âœ… ç¼“å­˜å‘½ä¸­: {cache_key}")
            return cache_result['data']
        elif cache_result['status'] == 'no_match':
            self.logger.info(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {cache_key}")
            return None
        else:
            # partial_match - æš‚æ—¶ä¸å¤„ç†ï¼Œè¿”å›Noneè®©è°ƒç”¨æ–¹é‡æ–°è·å–
            self.logger.info(f"âš ï¸ ç¼“å­˜éƒ¨åˆ†å‘½ä¸­: {cache_key}, éœ€è¦é‡æ–°è·å–")
            return None
    
    def save_data(self,
                  data: pd.DataFrame,
                  data_source: str,
                  market: str,
                  code: str,
                  start_date: date,
                  end_date: date,
                  interval: str = '1d') -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°ç¼“å­˜ï¼ˆå¸¦æ™ºèƒ½æ£€æŸ¥ï¼Œé¿å…é‡å¤ç¼“å­˜ï¼‰
        
        Args:
            data: è¦ç¼“å­˜çš„DataFrame
            data_source: æ•°æ®æºåç§°
            market: å¸‚åœºç±»å‹
            code: è‚¡ç¥¨/èµ„äº§ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            interval: æ—¶é—´ç²’åº¦
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        if not self.config.get("cache_settings", {}).get("enabled", True):
            return False
        
        if data is None or data.empty:
            self.logger.warning("æ•°æ®ä¸ºç©ºï¼Œä¸ä¿å­˜ç¼“å­˜")
            return False
        
        try:
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(data_source, market, code, start_date, end_date, interval)
            
            # ğŸ†• æ£€æŸ¥æ˜¯å¦å·²æœ‰èƒ½è¦†ç›–æ­¤èŒƒå›´çš„æ›´å¤§ç¼“å­˜
            # æå–æŸ¥è¯¢çš„åŸºæœ¬ä¿¡æ¯
            all_entries = self.index.get_all_entries()
            for existing_key, existing_entry in all_entries.items():
                # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªèµ„äº§
                if (existing_entry.get('data_source') == data_source and
                    existing_entry.get('market') == market and
                    existing_entry.get('code') == code and
                    existing_entry.get('interval') == interval):
                    
                    # æ£€æŸ¥ç°æœ‰ç¼“å­˜çš„æ—¥æœŸèŒƒå›´
                    existing_start = datetime.strptime(existing_entry['start_date'], '%Y-%m-%d').date()
                    existing_end = datetime.strptime(existing_entry['end_date'], '%Y-%m-%d').date()
                    
                    # å¦‚æœç°æœ‰ç¼“å­˜å®Œå…¨è¦†ç›–è¦ä¿å­˜çš„èŒƒå›´
                    if existing_start <= start_date and existing_end >= end_date:
                        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                        if not self.policy.is_expired(existing_entry):
                            self.logger.info(f"â­ï¸  è·³è¿‡ä¿å­˜: å·²æœ‰æ›´å¤§èŒƒå›´çš„ç¼“å­˜ ({existing_key}) è¦†ç›–æ­¤æŸ¥è¯¢")
                            return True  # è¿”å›Trueè¡¨ç¤ºä¸éœ€è¦ä¿å­˜ï¼ˆå·²æœ‰ç¼“å­˜ï¼‰
            
            # ä¿å­˜æ•°æ®æ–‡ä»¶
            file_path = self.storage.save(data, data_source, market, code, start_date, end_date, interval)
            
            if not file_path:
                self.logger.error(f"ä¿å­˜æ•°æ®æ–‡ä»¶å¤±è´¥: {cache_key}")
                return False
            
            # æ›´æ–°ç´¢å¼•
            metadata = {
                'file_path': str(file_path),
                'data_source': data_source,
                'market': market,
                'code': code,
                'start_date': start_date.strftime('%Y-%m-%d'),
                'end_date': end_date.strftime('%Y-%m-%d'),
                'interval': interval,
                'rows': len(data),
                'columns': list(data.columns),
                'created_at': datetime.now().isoformat(),
                'last_accessed': datetime.now().isoformat(),
                'access_count': 0,
                'file_size_kb': round(os.path.getsize(file_path) / 1024, 2),
                'checksum': self._calculate_checksum(file_path),
                'is_complete': True
            }
            
            self.index.add_entry(cache_key, metadata)
            
            self.logger.info(f"âœ… ç¼“å­˜ä¿å­˜æˆåŠŸ: {cache_key} ({metadata['file_size_kb']} KB)")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
            self._check_and_cleanup()
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _generate_cache_key(self, data_source: str, market: str, code: str,
                           start_date: date, end_date: date, interval: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        start_str = start_date.strftime('%Y%m%d')
        end_str = end_date.strftime('%Y%m%d')
        return f"{data_source}_{market}_{code}_{start_str}_{end_str}_{interval}"
    
    def _query_cache(self, cache_key: str, start_date: date, end_date: date) -> dict:
        """
        æŸ¥è¯¢ç¼“å­˜ï¼ˆæ”¯æŒæ™ºèƒ½æ—¥æœŸèŒƒå›´åŒ¹é…ï¼‰
        
        Returns:
            {
                'status': 'full_match' | 'partial_match' | 'no_match',
                'data': DataFrame or None,
                'caches': List of matching cache entries
            }
        """
        # 1. å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆæœ€å¿«ï¼‰
        if self.index.has_entry(cache_key):
            entry = self.index.get_entry(cache_key)
            result = self._check_and_load_cache(entry, cache_key, start_date, end_date)
            if result['status'] == 'full_match':
                return result
        
        # 2. ç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾èƒ½è¦†ç›–æŸ¥è¯¢èŒƒå›´çš„æ›´å¤§ç¼“å­˜
        # æå–æŸ¥è¯¢çš„åŸºæœ¬ä¿¡æ¯ï¼ˆæ•°æ®æºã€å¸‚åœºã€ä»£ç ã€æ—¶é—´ç²’åº¦ï¼‰
        key_parts = cache_key.split('_')
        if len(key_parts) >= 6:
            data_source = key_parts[0]
            market = key_parts[1]
            code = key_parts[2]
            # key_parts[3] = start_date, key_parts[4] = end_date
            interval = key_parts[5] if len(key_parts) > 5 else '1d'
            
            # éå†æ‰€æœ‰ç¼“å­˜ï¼ŒæŸ¥æ‰¾èƒ½è¦†ç›–æŸ¥è¯¢èŒƒå›´çš„ç¼“å­˜
            all_entries = self.index.get_all_entries()
            for existing_key, existing_entry in all_entries.items():
                # è·³è¿‡å·²æ£€æŸ¥çš„ç²¾ç¡®åŒ¹é…
                if existing_key == cache_key:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªèµ„äº§ï¼ˆæ•°æ®æºã€å¸‚åœºã€ä»£ç ã€æ—¶é—´ç²’åº¦ç›¸åŒï¼‰
                if (existing_entry.get('data_source') == data_source and
                    existing_entry.get('market') == market and
                    existing_entry.get('code') == code and
                    existing_entry.get('interval') == interval):
                    
                    # æ£€æŸ¥æ—¥æœŸèŒƒå›´æ˜¯å¦èƒ½è¦†ç›–æŸ¥è¯¢èŒƒå›´
                    result = self._check_and_load_cache(existing_entry, existing_key, start_date, end_date)
                    if result['status'] == 'full_match':
                        self.logger.info(f"âœ… æ‰¾åˆ°è¦†ç›–ç¼“å­˜: {existing_key} (è¦†ç›–æŸ¥è¯¢èŒƒå›´)")
                        return result
        
        # 3. æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„ç¼“å­˜
        return {'status': 'no_match', 'data': None, 'caches': []}
    
    def _check_and_load_cache(self, entry: dict, cache_key: str, start_date: date, end_date: date) -> dict:
        """
        æ£€æŸ¥å¹¶åŠ è½½ç¼“å­˜
        
        Args:
            entry: ç¼“å­˜æ¡ç›®
            cache_key: ç¼“å­˜é”®
            start_date: æŸ¥è¯¢å¼€å§‹æ—¥æœŸ
            end_date: æŸ¥è¯¢ç»“æŸæ—¥æœŸ
            
        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸
        """
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if self.policy.is_expired(entry):
            self.logger.info(f"ç¼“å­˜å·²è¿‡æœŸ: {cache_key}")
            return {'status': 'no_match', 'data': None, 'caches': []}
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_path = Path(entry['file_path'])
        if not file_path.exists():
            self.logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            self.index.remove_entry(cache_key)
            return {'status': 'no_match', 'data': None, 'caches': []}
        
        # è¯»å–æ•°æ®
        data = self.storage.load(file_path)
        if data is None:
            self.logger.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {file_path}")
            return {'status': 'no_match', 'data': None, 'caches': []}
        
        # æ£€æŸ¥æ—¥æœŸèŒƒå›´æ˜¯å¦å®Œå…¨åŒ…å«æŸ¥è¯¢èŒƒå›´
        cache_start = datetime.strptime(entry['start_date'], '%Y-%m-%d').date()
        cache_end = datetime.strptime(entry['end_date'], '%Y-%m-%d').date()
        
        if cache_start <= start_date and cache_end >= end_date:
            # âœ… ç¼“å­˜èŒƒå›´å®Œå…¨è¦†ç›–æŸ¥è¯¢èŒƒå›´ï¼Œè¿‡æ»¤æ•°æ®
            filtered_data = data[(data.index.date >= start_date) & (data.index.date <= end_date)]
            
            if filtered_data.empty:
                self.logger.warning(f"è¿‡æ»¤åæ•°æ®ä¸ºç©º: {cache_key}")
                return {'status': 'no_match', 'data': None, 'caches': []}
            
            # æ›´æ–°è®¿é—®è®°å½•
            self.index.update_access(cache_key)
            
            self.logger.info(f"âœ… ä»ç¼“å­˜è¿‡æ»¤æ•°æ®: {len(filtered_data)} æ¡è®°å½• (åŸç¼“å­˜: {len(data)} æ¡)")
            
            return {
                'status': 'full_match',
                'data': filtered_data,
                'caches': [entry],
                'from_larger_cache': cache_key != f"{entry['data_source']}_{entry['market']}_{entry['code']}_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}_{entry['interval']}"
            }
        
        # æ—¥æœŸèŒƒå›´ä¸åŒ¹é…
        return {'status': 'no_match', 'data': None, 'caches': []}
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        try:
            md5_hash = hashlib.md5()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    md5_hash.update(chunk)
            return f"md5:{md5_hash.hexdigest()}"
        except Exception as e:
            self.logger.warning(f"è®¡ç®—æ ¡éªŒå’Œå¤±è´¥: {e}")
            return "md5:unknown"
    
    def _check_and_cleanup(self):
        """æ£€æŸ¥ç¼“å­˜å¤§å°å¹¶æ¸…ç†"""
        stats = self.index.get_statistics()
        max_size_mb = self.config.get("cache_settings", {}).get("max_size_mb", 1024)
        
        if stats['total_size_mb'] > max_size_mb:
            self.logger.warning(f"ç¼“å­˜è¶…è¿‡é™åˆ¶ ({stats['total_size_mb']:.2f} MB > {max_size_mb} MB)ï¼Œå¼€å§‹æ¸…ç†")
            self.cleanup_cache()
    
    def cleanup_cache(self, force: bool = False):
        """
        æ¸…ç†ç¼“å­˜
        
        Args:
            force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†ï¼ˆå¿½ç•¥è®¿é—®è®°å½•ï¼‰
        """
        self.logger.info("å¼€å§‹æ¸…ç†ç¼“å­˜...")
        
        entries = self.index.get_all_entries()
        to_delete = []
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        for key, entry in entries.items():
            if self.policy.is_expired(entry):
                to_delete.append(key)
        
        # å¦‚æœè¿˜éœ€è¦æ¸…ç†æ›´å¤šï¼ˆè¶…è¿‡å®¹é‡é™åˆ¶ï¼‰
        stats = self.index.get_statistics()
        max_size_mb = self.config.get("cache_settings", {}).get("max_size_mb", 1024)
        
        if stats['total_size_mb'] > max_size_mb or force:
            # LRUç­–ç•¥ï¼šåˆ é™¤æœ€ä¹…æœªè®¿é—®çš„
            sorted_entries = sorted(
                entries.items(),
                key=lambda x: x[1].get('last_accessed', ''),
                reverse=False  # æœ€æ—§çš„åœ¨å‰
            )
            
            for key, entry in sorted_entries:
                if key not in to_delete:
                    to_delete.append(key)
                    if self.index.get_statistics()['total_size_mb'] <= max_size_mb * 0.8:
                        break
        
        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        for key in to_delete:
            if self.delete_cache(key):
                deleted_count += 1
        
        self.logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªç¼“å­˜")
    
    def delete_cache(self, cache_key: str) -> bool:
        """åˆ é™¤æŒ‡å®šç¼“å­˜"""
        try:
            entry = self.index.get_entry(cache_key)
            if entry:
                file_path = Path(entry['file_path'])
                if file_path.exists():
                    file_path.unlink()
                    self.logger.info(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶: {file_path}")
            
            self.index.remove_entry(cache_key)
            return True
        except Exception as e:
            self.logger.error(f"åˆ é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def clear_all_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.logger.warning("æ¸…ç©ºæ‰€æœ‰ç¼“å­˜...")
        entries = self.index.get_all_entries()
        for key in list(entries.keys()):
            self.delete_cache(key)
        self.logger.info("æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")
    
    def get_statistics(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self.index.get_statistics()


class CacheIndex:
    """ç¼“å­˜ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self, index_file: Path):
        """
        åˆå§‹åŒ–ç´¢å¼•ç®¡ç†å™¨
        
        Args:
            index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„
        """
        self.index_file = index_file
        self.index_file.parent.mkdir(parents=True, exist_ok=True)
        self.data = self._load_index()
    
    def _load_index(self) -> dict:
        """åŠ è½½ç´¢å¼•æ–‡ä»¶"""
        if not self.index_file.exists():
            return {
                "version": "1.0",
                "last_update": "",
                "entries": {},
                "statistics": {
                    "total_entries": 0,
                    "total_size_mb": 0,
                    "oldest_entry": "",
                    "newest_entry": ""
                }
            }
        
        try:
            with open(self.index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"åŠ è½½ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")
            return self._load_index()  # è¿”å›é»˜è®¤å€¼
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            self.data['last_update'] = datetime.now().isoformat()
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ä¿å­˜ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")
    
    def has_entry(self, key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨æŒ‡å®šç¼“å­˜"""
        return key in self.data['entries']
    
    def get_entry(self, key: str) -> Optional[dict]:
        """è·å–ç¼“å­˜æ¡ç›®"""
        return self.data['entries'].get(key)
    
    def add_entry(self, key: str, metadata: dict):
        """æ·»åŠ ç¼“å­˜æ¡ç›®"""
        self.data['entries'][key] = metadata
        self._update_statistics()
        self._save_index()
    
    def remove_entry(self, key: str):
        """åˆ é™¤ç¼“å­˜æ¡ç›®"""
        if key in self.data['entries']:
            del self.data['entries'][key]
            self._update_statistics()
            self._save_index()
    
    def update_access(self, key: str):
        """æ›´æ–°è®¿é—®è®°å½•"""
        if key in self.data['entries']:
            self.data['entries'][key]['last_accessed'] = datetime.now().isoformat()
            self.data['entries'][key]['access_count'] = self.data['entries'][key].get('access_count', 0) + 1
            self._save_index()
    
    def get_all_entries(self) -> dict:
        """è·å–æ‰€æœ‰ç¼“å­˜æ¡ç›®"""
        return self.data['entries']
    
    def _update_statistics(self):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        entries = self.data['entries']
        
        if not entries:
            self.data['statistics'] = {
                "total_entries": 0,
                "total_size_mb": 0,
                "oldest_entry": "",
                "newest_entry": ""
            }
            return
        
        total_size_kb = sum(entry.get('file_size_kb', 0) for entry in entries.values())
        created_times = [entry.get('created_at', '') for entry in entries.values() if entry.get('created_at')]
        
        self.data['statistics'] = {
            "total_entries": len(entries),
            "total_size_mb": round(total_size_kb / 1024, 2),
            "oldest_entry": min(created_times) if created_times else "",
            "newest_entry": max(created_times) if created_times else ""
        }
    
    def get_statistics(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.data['statistics']


class CacheStorage:
    """ç¼“å­˜å­˜å‚¨å±‚ - è´Ÿè´£æ–‡ä»¶è¯»å†™"""
    
    def __init__(self, data_dir: Path, config: dict):
        """
        åˆå§‹åŒ–å­˜å‚¨å±‚
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            config: é…ç½®å­—å…¸
        """
        self.data_dir = data_dir
        self.config = config
        self.format = config.get('storage_format', {}).get('format', 'parquet')
        self.compression = config.get('storage_format', {}).get('compression', 'snappy')
    
    def save(self, data: pd.DataFrame, data_source: str, market: str, code: str,
             start_date: date, end_date: date, interval: str) -> Optional[Path]:
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
        
        Returns:
            æ–‡ä»¶è·¯å¾„æˆ–None
        """
        try:
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            subdir = self.data_dir / data_source / market
            subdir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            start_str = start_date.strftime('%Y%m%d')
            end_str = end_date.strftime('%Y%m%d')
            
            if interval == '1d':
                filename = f"{code}_{start_str}_{end_str}.{self.format}"
            else:
                filename = f"{code}_{start_str}_{end_str}_{interval}.{self.format}"
            
            file_path = subdir / filename
            
            # ä¿å­˜æ–‡ä»¶
            if self.format == 'parquet':
                data.to_parquet(file_path, compression=self.compression)
            elif self.format == 'csv':
                data.to_csv(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å­˜å‚¨æ ¼å¼: {self.format}")
            
            return file_path
            
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def load(self, file_path: Path) -> Optional[pd.DataFrame]:
        """
        ä»æ–‡ä»¶åŠ è½½æ•°æ®
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            DataFrameæˆ–None
        """
        try:
            if not file_path.exists():
                return None
            
            if file_path.suffix == '.parquet':
                return pd.read_parquet(file_path)
            elif file_path.suffix == '.csv':
                df = pd.read_csv(file_path, index_col=0, parse_dates=True)
                return df
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                
        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            return None


class CachePolicy:
    """ç¼“å­˜ç­–ç•¥ç®¡ç†å™¨ - è´Ÿè´£TTLå’Œæ¸…ç†ç­–ç•¥"""
    
    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–ç­–ç•¥ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
    
    def is_expired(self, entry: dict) -> bool:
        """
        åˆ¤æ–­ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        
        Args:
            entry: ç¼“å­˜æ¡ç›®
            
        Returns:
            æ˜¯å¦è¿‡æœŸ
        """
        try:
            # è·å–ç¼“å­˜ç»“æŸæ—¥æœŸ
            end_date_str = entry.get('end_date', '')
            if not end_date_str:
                return True
            
            end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()
            today = date.today()
            days_diff = (today - end_date).days
            
            # TTLè§„åˆ™
            ttl_rules = self.config.get('ttl_rules', {})
            
            # å†å²æ•°æ®ï¼ˆç»“æŸæ—¥æœŸ > 7å¤©å‰ï¼‰ï¼šæ°¸ä¸è¿‡æœŸ
            recent_days = ttl_rules.get('recent_data_days', 7)
            if days_diff > recent_days:
                return False  # æ°¸ä¸è¿‡æœŸ
            
            # è¿‘æœŸæ•°æ®ï¼ˆç»“æŸæ—¥æœŸåœ¨1-7å¤©å‰ï¼‰ï¼š7å¤©è¿‡æœŸ
            if 1 <= days_diff <= recent_days:
                created_at_str = entry.get('created_at', '')
                if created_at_str:
                    created_at = datetime.fromisoformat(created_at_str)
                    ttl_hours = ttl_rules.get('recent_data_ttl_hours', 168)
                    expire_time = created_at + timedelta(hours=ttl_hours)
                    return datetime.now() > expire_time
                return False
            
            # æœ€æ–°æ•°æ®ï¼ˆç»“æŸæ—¥æœŸæ˜¯ä»Šå¤©ï¼‰ï¼š1å°æ—¶è¿‡æœŸ
            if days_diff == 0:
                created_at_str = entry.get('created_at', '')
                if created_at_str:
                    created_at = datetime.fromisoformat(created_at_str)
                    
                    # åŠ å¯†è´§å¸ï¼š30åˆ†é’Ÿ
                    market = entry.get('market', '')
                    if market == 'crypto':
                        ttl_minutes = ttl_rules.get('crypto_ttl_minutes', 30)
                        expire_time = created_at + timedelta(minutes=ttl_minutes)
                    else:
                        ttl_minutes = ttl_rules.get('realtime_data_ttl_minutes', 60)
                        expire_time = created_at + timedelta(minutes=ttl_minutes)
                    
                    return datetime.now() > expire_time
                return False
            
            return False
            
        except Exception as e:
            print(f"åˆ¤æ–­ç¼“å­˜è¿‡æœŸå¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶è®¤ä¸ºè¿‡æœŸ
